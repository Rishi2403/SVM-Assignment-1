{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "What is the mathematical formula for a linear SVM?"
      ],
      "metadata": {
        "id": "vW_VwSU1GN2x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### New SectionThe mathematical formula for a linear SVM can be represented as:\n",
        "\n",
        "f(x) = sign(W^T * X + b)\n",
        "\n",
        "Where:\n",
        "- f(x) is the decision function that predicts the class label of a new input data point x\n",
        "- W is the weight vector\n",
        "- X is the input data point\n",
        "- b is the bias term\n",
        "- sign() is the sign function that returns +1 if the input is positive, -1 if the input is negative\n",
        "\n",
        "The goal of the SVM algorithm is to find the optimal values of W and b that maximize the margin between the support vectors of different classes. This is done by solving the optimization problem:\n",
        "\n",
        "minimize 1/2 * ||W||^2\n",
        "subject to: y_i * (W^T * x_i + b) >= 1 for all training data points (x_i, y_i)\n",
        "\n",
        "Where:\n",
        "- y_i is the class label of the training data point x_i\n",
        "- ||W||^2 is the squared norm of the weight vector\n",
        "\n",
        "By solving this optimization problem, we can find the optimal hyperplane that separates the two classes with the maximum margin."
      ],
      "metadata": {
        "id": "hP4qZACeGhfe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the kernel trick in SVM?\n"
      ],
      "metadata": {
        "id": "cg7oDSJDGtVW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The kernel trick in SVM is a technique used to handle non-linearly separable data by implicitly mapping the input features into a higher-dimensional space where the data points become linearly separable. This is achieved by defining a kernel function that computes the dot product of the input data points in this higher-dimensional space without explicitly transforming the data.\n",
        "\n",
        "The kernel function allows the SVM algorithm to operate in this higher-dimensional space without actually computing the transformation, which can be computationally expensive for high-dimensional data. By using the kernel trick, the SVM can find the optimal hyperplane that separates the classes in the higher-dimensional space, even if the original data points are not linearly separable.\n",
        "\n",
        "Some commonly used kernel functions in SVM include:\n",
        "1. Linear kernel: K(x, x') = x^T * x'\n",
        "2. Polynomial kernel: K(x, x') = (gamma * x^T * x' + r)^d\n",
        "3. Radial Basis Function (RBF) kernel: K(x, x') = exp(-gamma * ||x - x'||^2)\n",
        "4. Sigmoid kernel: K(x, x') = tanh(gamma * x^T * x' + r)\n",
        "\n",
        "# By using different kernel functions, the SVM algorithm can effectively handle non-linear decision boundaries and classify complex data sets that cannot be separated by a linear hyperplane in the original feature space. The kernel trick is a powerful technique that enhances the flexibility and performance of SVM in handling non-linear classification problems."
      ],
      "metadata": {
        "id": "cNrx7R0RGtQ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the role of support vectors in SVM Explain with example"
      ],
      "metadata": {
        "id": "xE_rFWFfGtM-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In SVM, support vectors are the data points that lie closest to the decision boundary (hyperplane) and have a non-zero value for the Lagrange multiplier. These support vectors play a crucial role in defining the decision boundary and determining the margin of the SVM classifier."
      ],
      "metadata": {
        "id": "tw_ra5_aHtYn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fjTvLaehHmyU"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "dMgjagftGtJG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jznBML-oGtFU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "T5FlPxpmGtBX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "QlLlnfIuGs9m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ejC-zlfdGDpG"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n"
      ]
    }
  ]
}